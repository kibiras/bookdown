[["index.html", "FMISD19004 Cloud Computing Technologies Introduction", " FMISD19004 Cloud Computing Technologies Kstutis Daugla 2021-11-01 Introduction The underlying concept of cloud computing was introduced way back in 1960s by John McCarthy in his book, The challenge of the Computer Utility. His opinion was that computation may someday be organized as a public utility. The rest became history and the majority of the sofware used now is running in the cloud seamlessly (Surbiryala and Rong 2019). The history of the cloud - image source https://itchronicles.com/ Cloud can solve a lot of problems nowadays - starting with reduced cost, enhanced security and flexible approach (Srivastava and Khan 2018) up to sustainability (Parthasarathy and Kumar 2012) and accessibility around the world. Continuous Integration and Deployment (CI/CD) is easier then even treating now only the applications, but the whole infrastructure as code. This leads to enhanced productivity and cost optimization (Garg and Garg 2019). Is there anything revolutional in the cloud offerings today? Definitely no, SaaS (Software as a service) is a software distribution model in which a cloud provider hosts applications and makes them available to end users over the internet PaaS (Platform as a service) is a complete development and deployment environment in the cloud, with resources that enable you to deliver everything from simple cloud-based apps to sophisticated, cloud-enabled enterprise applications IaaS (Infrastructure as a service) is a type of cloud computing service that offers essential compute, storage, and networking resources on demand, on a pay-as-you-go basis IaaS vs PaaS vs SaaS - image source https://www.bigcommerce.com/blog/saas-vs-paas-vs-iaas/ However, despite the gain achieved from cloud computing, the organizations are slow in accepting it due to security issues and challenges associated with it (Bairagi and Bang 2015). Leaders in Cloud Infrastructure and Services Magic Quadrant for Cloud Infrastructure and Platform Services References "],["moving-to-cloud.html", "Chapter 1 Moving to cloud 1.1 Managing SLA (SLO) requirements on clouds through scalability and reliability 1.2 Migration to cloud approaches 1.3 Kubernetes in a nutshell 1.4 Use case", " Chapter 1 Moving to cloud 1.1 Managing SLA (SLO) requirements on clouds through scalability and reliability The big challenge for Cloud customers is to evaluate SLAs of cloud vendors. (Odun-Ayo, Ajayi, and Omoregbe 2017) (Aljoumah et al. 2015) There are two types of SLAs in a cloud computing environment: SLAs between the end user and the cloud client, and SLAs between the cloud client and the cloud infrastructure provider. This paper investigates the cost/performance trade-off from the cloud clients perspective. From the cloud clients point-of-view the auto-scaling goal is to reduce resource cost (i.e., the cost of the leased resources from the IaaS provider) and the SLA violation cost (i.e., the cost that is associated with the SLA breaches), at the same time. Public cloud Most services are offered in a public environment in which consumers can access a resource pool that is managed by a host corporation [14]. Because of its existence, this type of environment will pose important concerns regarding security issues [15]. Private cloud A third party vendor provides the services which distinguish it from public accesses [16]. Therefore it is better than the previous development model because it prevents unauthorized access. Community cloud The cloud services are provided to a specified group where all members are entitled to equal access to the sheared services [17]. Hybrid cloud The cloud services are provided as multiple cloud combustion (public cloud, private cloud, and community cloud) [18]. It might just inherit any kind of vulnerability or risk which resides within the parties listed above Security requirements represent a major issue that has to be met in order of easing some of these obstacles (Alam 2020) 1.2 Migration to cloud approaches . This means that all applications moving into the cloud must run in a virtualized way. Secondly, cloud computing advocates that resources should be controlled on demand, and can be flexibly and elastically expanded and contracted according to the change of demand. This means that the business of the programs running on the cloud will work in a parallel and cluster mode. However, not all the existing application architectures are designed in a parallel or cluster mode. Of course, virtual machines also become the most direct entry for applications running in the cloud. PaaS level services provide platform level services, such as development, deployment, monitoring and other tools in the cloud environment and platform level software in other clouds, to help applications more easily, quickly and effectively migrate to the cloud. On the other hand, most of the existing applications cannot run directly in the cloud environment due to their own architecture differences or other reasons. (Wang, Yan, and Wang 2020) According to Forbes, there are now 77 % of organizations, having one or some parts of their systems in the cloud, stated as The trust deficit between client and cloud providers regarding a set of security protocols is also part of the findings. Sometimes many organizations do not want to invest in new technology components as they think it as an additional cost (Iqbal and Colomo-Palacios 2019) 1.3 Kubernetes in a nutshell Kubernetes (, Greek for helmsman or pilot or governor, and the etymological root of cybernetics)[6] was founded by Ville Aikas, Joe Beda, Brendan Burns, and Craig McLuckie,[12] who were quickly joined by other Google engineers including Brian Grant and Tim Hockin, and was first announced by Google in mid-2014.[13] Its development and design are heavily influenced by Googles Borg system,[14][15] and many of the top contributors to the project previously worked on Borg. The original codename for Kubernetes within Google was Project 7, a reference to the Star Trek ex-Borg character Seven of Nine.[16] The seven spokes on the wheel of the Kubernetes logo are a reference to that codename. The original Borg project was written entirely in C++,[14] but the rewritten Kubernetes system is implemented in Go. Its goals are to build on the capabilities of containers to provide significant gains in programmer productivity and ease of both manual and automated system management. (Verma et al. 2015) (Burns et al. 2016) Traditional deployment era: Early on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers. Virtualized deployment era: As a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical servers CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application. Virtualization allows better utilization of resources in a physical server and allows better scalability because an application can be added or updated easily, reduces hardware costs, and much more. With virtualization you can present a set of physical resources as a cluster of disposable virtual machines. Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware. Container deployment era: Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications. Therefore, containers are considered lightweight. Similar to a VM, a container has its own filesystem, share of CPU, memory, process space, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions. Container evolution - kubernetes.io Kubernetes is the most popular container orchestration platform that enables users to create and run multiple containers in cloud environments. Kubernetes offers resource management to isolate the resource usage of containers on a host server because performance isolation is an important factor in terms of service quality. The components of a Kubernetes cluster - kubernetes.io Control Plane Components The control planes components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events (for example, starting up a new pod when a deployments replicas field is unsatisfied). Control plane components can be run on any machine in the cluster. However, for simplicity, set up scripts typically start all control plane components on the same machine, and do not run user containers on this machine. See Creating Highly Available clusters with kubeadm for an example control plane setup that runs across multiple VMs. kube-apiserver The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. The main implementation of a Kubernetes API server is kube-apiserver. kube-apiserver is designed to scale horizontallythat is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances. etcd Consistent and highly-available key value store used as Kubernetes backing store for all cluster data. If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for those data. You can find in-depth information about etcd in the official documentation. kube-scheduler Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include: individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and deadlines. kube-controller-manager Control plane component that runs controller processes. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process. Some types of these controllers are: Node controller: Responsible for noticing and responding when nodes go down. Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion. Endpoints controller: Populates the Endpoints object (that is, joins Services &amp; Pods). Service Account &amp; Token controllers: Create default accounts and API access tokens for new namespaces. cloud-controller-manager A Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud providers API, and separates out the components that interact with that cloud platform from components that only interact with your cluster. The cloud-controller-manager only runs controllers that are specific to your cloud provider. If you are running Kubernetes on your own premises, or in a learning environment inside your own PC, the cluster does not have a cloud controller manager. As with the kube-controller-manager, the cloud-controller-manager combines several logically independent control loops into a single binary that you run as a single process. You can scale horizontally (run more than one copy) to improve performance or to help tolerate failures. The following controllers can have cloud provider dependencies: Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding Route controller: For setting up routes in the underlying cloud infrastructure Service controller: For creating, updating and deleting cloud provider load balancers Node Components Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment. kubelet An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesnt manage containers which were not created by Kubernetes. kube-proxy kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and its available. Otherwise, kube-proxy forwards the traffic itself. Container runtime The container runtime is the software that is responsible for running containers. Kubernetes supports several container runtimes: Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI (Container Runtime Interface). Addons Addons use Kubernetes resources (DaemonSet, Deployment, etc) to implement cluster features. Because these are providing cluster-level features, namespaced resources for addons belong within the kube-system namespace. Selected addons are described below; for an extended list of available addons, please see Addons. DNS While the other addons are not strictly required, all Kubernetes clusters should have cluster DNS, as many examples rely on it. Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server in their DNS searches. Web UI (Dashboard) Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications running in the cluster, as well as the cluster itself. Container Resource Monitoring Container Resource Monitoring records generic time-series metrics about containers in a central database, and provides a UI for browsing that data. Cluster-level Logging 1.4 Use case R programming language and its ecosystem. The philosophy of docker is to encapsulate software code and all its dependencies so that it can run uniformly and consistently on any infrastructure, namely, the containerization strategy. R Shiny apps usually have many package dependencies. Well first test its running on a local computer and then make them work remotely. Without containerization by docker, well have to make sure all the computers that run the apps have the same running environment. That means well spend much effort installing and configuring numerous software and packages. And when a package gets updated on the test computer, it has to be updated in all the other ones. We can see it will easily become painful to manage multiple R shiny apps in this way. With docker, we can pack the apps, their dependencies, and the running environment as a whole image. The image can be copied to other computers and we can just start running the apps (as long as that computer has docker installed). Without being disrupted by tedious installations and configurations, this type of practice greatly speeds up the workflow of software development and deployment. https://towardsdatascience.com/an-open-source-solution-to-deploy-enterprise-level-r-shiny-applications-2e19d950ff35 When it comes to managed Kubernetes services, Google Kubernetes Engine (GKE) is a great choice if you are looking for a container orchestration platform that offers advanced scalability and configuration flexibility. GKE gives you complete control over every aspect of container orchestration, from networking, to storage, to how you set up observabilityin addition to supporting stateful application use cases. However, if your application does not need that level of cluster configuration and monitoring, then fully managed Cloud Run might be the right solution for you. Cloud Run is intended for those who focus on container-based development versus using source-based systems. The idea is that containers provide separation of duties between the developer and the platform where the container executes. The serverless benefits are also compelling. While some people can accurately size resources, and even leverage reserved instances at a discount, most enterprises struggle to manage cloud operations within cost parameters. Serverless provides more fine-grained billing. Reason to choose Cloud Run i/o Kubernetes With a manually created GKE cluster, the nodes and environment are always on which means that you are billed for them regardless of utilization. With Cloud Run, your service is merely available and you are only billed for actual consumption. If your service not being called, your costs are zero. Another advantage is that you dont have to predict your utilization needs and allocate sufficient nodes. Scaling happens automatically for you. https://stackoverflow.com/questions/55786955/whats-the-value-proposition-of-running-cloud-run-versus-a-normal-service-in-gke Easy deployment of microservices: Deploy a containerized microservice with a single command without requiring any additional service-specific configuration. Simple and unified developer experience: Helps to quickly deploy and manage services that automatically scales up depending on traffic. Run HTTP services: Allow to create HTTP services that respond to all normal HTTP methods like POST, GET, etc. Payperuse: Only pay when your code is running Support for code written in any language: Cloud Run relies on containers, thus youll be able to write code in any language, exploitation any binary and framework. Integrated logging and monitoring: To ensure the health of an application, it is integrated with Stackdriver Monitoring, Logging, and Error Reporting Knative compatible: Cloud Run is constructed on the Knative opensource project, enabling portability of your workloads across platforms. https://tudip.com/blog-post/gke-or-cloud-run-which-should-you-use/ Google Kubernetes Engine (GKE) provides a managed environment for deploying, managing, and scaling your containerized applications using Google infrastructure. Cloud Run is a service by Google Cloud Platform to run your stateless HTTP containers without worrying about provisioning machines, clusters or auto-scaling. https://medium.com/google-cloud/continuous-deployment-to-cloud-run-on-google-kubernetes-engine-ebe49bd956bf Cloud Run Process - https://medium.com/google-cloud https://code.markedmondson.me/r-on-kubernetes-serverless-shiny-r-apis-and-scheduled-scripts/ https://www.linkedin.com/pulse/using-kubernetes-deploy-r-shiny-application-nosa-ugowe/ References "],["part-ii-tba.html", "Chapter 2 Part II (TBA) 2.1 Networking 2.2 Security 2.3 Monitoring 2.4 Use Case", " Chapter 2 Part II (TBA) 2.1 Networking 2.2 Security 2.3 Monitoring 2.4 Use Case "],["part-iii-tba.html", "Chapter 3 Part III (TBA) 3.1 Infrastructure as Code 3.2 Data Governance 3.3 Use Case", " Chapter 3 Part III (TBA) 3.1 Infrastructure as Code 3.2 Data Governance 3.3 Use Case "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
